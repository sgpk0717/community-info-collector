#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ê°€ì¤‘ì¹˜ ê²€ìƒ‰ - í‚¤ì›Œë“œë³„ ì¤‘ìš”ë„ ë°˜ì˜
"""
import os
import sys
import json
from datetime import datetime
from dotenv import load_dotenv
import time

# ìœˆë„ìš° í™˜ê²½ì—ì„œ ìœ ë‹ˆì½”ë“œ ì¶œë ¥ ì„¤ì •
if sys.platform == 'win32':
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer)
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer)

sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
load_dotenv()

from app.services.reddit_service import RedditService
from openai import OpenAI

def expand_keywords_with_gpt4(user_input):
    """GPT-4ë¥¼ ì‚¬ìš©í•˜ì—¬ í‚¤ì›Œë“œë¥¼ í™•ì¥í•˜ê³  ì˜ì–´ë¡œ ë³€í™˜"""
    try:
        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        
        prompt = f"""
ì‚¬ìš©ìê°€ ì…ë ¥í•œ í‚¤ì›Œë“œ: "{user_input}"

ì´ í‚¤ì›Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ Redditì—ì„œ ìµœì‹  ì •ë³´ì™€ ì°Œë¼ì‹œ, ë£¨ë¨¸, ë‰´ìŠ¤ë¥¼ ì˜ ì°¾ì„ ìˆ˜ ìˆë„ë¡ 6ê°œì˜ ì˜ì–´ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”.

ìš”êµ¬ì‚¬í•­:
1. ì²« ë²ˆì§¸ í‚¤ì›Œë“œëŠ” ì›ë˜ í‚¤ì›Œë“œì˜ ì§ì ‘ ë²ˆì—­
2. ë‚˜ë¨¸ì§€ 5ê°œëŠ” ê´€ë ¨ëœ ìµœì‹  ì •ë³´, ë‰´ìŠ¤, ë£¨ë¨¸, ë¶„ì„ì„ ì˜ ì°¾ì„ ìˆ˜ ìˆëŠ” í‚¤ì›Œë“œ
3. ê° í‚¤ì›Œë“œëŠ” Reddit ê²€ìƒ‰ì— ì í•©í•˜ë„ë¡ 2-4ê°œ ë‹¨ì–´ë¡œ êµ¬ì„±
4. íˆ¬ì, ì£¼ì‹, ê¸°ì—… ê´€ë ¨ì´ë©´ earnings, forecast, analysis, news, rumor ë“± í¬í•¨
5. ì¸ë¬¼ ê´€ë ¨ì´ë©´ controversy, scandal, latest, update ë“± í¬í•¨

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{{
    "keywords": [
        {{"keyword": "í‚¤ì›Œë“œ1", "reason": "ì„ íƒ ì´ìœ "}},
        {{"keyword": "í‚¤ì›Œë“œ2", "reason": "ì„ íƒ ì´ìœ "}},
        {{"keyword": "í‚¤ì›Œë“œ3", "reason": "ì„ íƒ ì´ìœ "}},
        {{"keyword": "í‚¤ì›Œë“œ4", "reason": "ì„ íƒ ì´ìœ "}},
        {{"keyword": "í‚¤ì›Œë“œ5", "reason": "ì„ íƒ ì´ìœ "}},
        {{"keyword": "í‚¤ì›Œë“œ6", "reason": "ì„ íƒ ì´ìœ "}}
    ]
}}
"""
        
        response = client.chat.completions.create(
            model="gpt-4.1",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7
        )
        
        # JSON ì‘ë‹µ íŒŒì‹±
        import json
        result = json.loads(response.choices[0].message.content)
        
        print(f"ğŸ¤– GPT-4.1ì´ ìƒì„±í•œ í™•ì¥ í‚¤ì›Œë“œ:")
        print("="*70)
        
        expanded_keywords = []
        for i, item in enumerate(result["keywords"], 1):
            expanded_keywords.append({
                "rank": i,
                "query": item["keyword"],
                "posts_to_collect": 10,
                "reason": item["reason"]
            })
            print(f"{i}. \"{item['keyword']}\" - {item['reason']}")
        
        print(f"\nì´ ëª©í‘œ ìˆ˜ì§‘ëŸ‰: {len(expanded_keywords) * 10}ê°œ")
        print("="*70)
        
        return expanded_keywords
        
    except Exception as e:
        print(f"âŒ GPT-4 í‚¤ì›Œë“œ í™•ì¥ ì‹¤íŒ¨: {e}")
        # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë³€í™˜ ë¡œì§ ì‚¬ìš©
        return [{"rank": 1, "query": translate_to_english_keywords(user_input), "posts_to_collect": 10, "reason": "ê¸°ë³¸ ë²ˆì—­"}]

def translate_to_english_keywords(korean_input):
    """í•œêµ­ì–´ í‚¤ì›Œë“œë¥¼ ì˜ì–´ë¡œ ë³€í™˜"""
    # ì£¼ìš” í‚¤ì›Œë“œ ë§¤í•‘
    keyword_mapping = {
        "ì• í”Œ": "Apple",
        "ì£¼ê°€": "stock price",
        "ì „ë§": "outlook forecast",
        "ì‚¼ì„±": "Samsung",
        "ë„¤ì´ë²„": "Naver",
        "ì¹´ì¹´ì˜¤": "Kakao",
        "LG": "LG",
        "í˜„ëŒ€": "Hyundai",
        "ê¸°ì•„": "Kia",
        "SK": "SK",
        "ë¶„ì„": "analysis",
        "íˆ¬ì": "investment",
        "ìˆ˜ìµ": "profit earnings",
        "ì‹¤ì ": "performance earnings",
        "ë§¤ì¶œ": "revenue sales",
        "ë°°ë‹¹": "dividend",
        "ìƒìŠ¹": "rise increase",
        "í•˜ë½": "fall decrease",
        "ì˜ˆì¸¡": "prediction forecast",
        "ì „ë§": "outlook forecast",
        "ë‰´ìŠ¤": "news",
        "ë°œí‘œ": "announcement",
        "ì‹¤ì ": "earnings results"
    }
    
    # í•œêµ­ì–´ í‚¤ì›Œë“œë¥¼ ì˜ì–´ë¡œ ë³€í™˜
    english_keywords = []
    for word in korean_input.split():
        if word in keyword_mapping:
            english_keywords.append(keyword_mapping[word])
        else:
            english_keywords.append(word)
    
    return " ".join(english_keywords)

def weighted_search(user_input=None):
    """ê°€ì¤‘ì¹˜ ê¸°ë°˜ ê²€ìƒ‰ - ì¤‘ìš”ë„ ìˆœìœ„ë³„ ê²Œì‹œë¬¼ ìˆ˜ì§‘"""
    reddit = RedditService()
    
    if not reddit.reddit:
        print("âŒ Reddit ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨!")
        return
    
    # ì‚¬ìš©ì ì…ë ¥ì´ ìˆìœ¼ë©´ í‚¤ì›Œë“œ ìƒì„±, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
    if user_input:
        # GPT-4ë¥¼ ì‚¬ìš©í•˜ì—¬ í‚¤ì›Œë“œ í™•ì¥
        print(f"ğŸ¤– GPT-4ë¡œ í‚¤ì›Œë“œ í™•ì¥ ì¤‘...")
        search_keywords = expand_keywords_with_gpt4(user_input)
    else:
        # ê¸°ë³¸ íŠ¸ëŸ¼í”„-ë¨¸ìŠ¤í¬ í‚¤ì›Œë“œ
        search_keywords = [
            {
                "rank": 1,
                "query": "Trump Musk conflict 2025",
                "posts_to_collect": 20,
                "reason": "ìµœì‹  ê°ˆë“± ìƒí™©ì„ ì§ì ‘ì ìœ¼ë¡œ ë‹¤ë£¨ëŠ” í‚¤ì›Œë“œ"
            },
            {
                "rank": 2,
                "query": "Musk America Party Trump",
                "posts_to_collect": 15,
                "reason": "ë¨¸ìŠ¤í¬ì˜ ìƒˆë¡œìš´ ì •ë‹¹ ì°½ë‹¹ê³¼ íŠ¸ëŸ¼í”„ì™€ì˜ ëŒ€ë¦½"
            },
            {
                "rank": 3,
                "query": "Trump vs Elon Musk feud",
                "posts_to_collect": 12,
                "reason": "ë‘ ì¸ë¬¼ ê°„ ë¶ˆí™”ì™€ ë…¼ìŸ"
            },
            {
                "rank": 4,
                "query": "Musk criticizes Trump GOP",
                "posts_to_collect": 8,
                "reason": "ë¨¸ìŠ¤í¬ì˜ íŠ¸ëŸ¼í”„/ê³µí™”ë‹¹ ë¹„íŒ"
            },
            {
                "rank": 5,
                "query": "Trump Musk relationship news",
                "posts_to_collect": 5,
                "reason": "ê´€ê³„ ë³€í™”ì— ëŒ€í•œ ì¼ë°˜ì  ë‰´ìŠ¤"
            }
        ]
    
    topic = user_input if user_input else "Trump vs Musk"
    print(f"ğŸ” {topic} ê°€ì¤‘ì¹˜ ê²€ìƒ‰")
    print("ğŸ“Š í‚¤ì›Œë“œë³„ ì¤‘ìš”ë„ ë° ìˆ˜ì§‘ ê³„íš:")
    print("="*70)
    for kw in search_keywords:
        print(f"{kw['rank']}ìˆœìœ„: \"{kw['query']}\" - {kw['posts_to_collect']}ê°œ")
        print(f"   ì´ìœ : {kw['reason']}")
    print(f"\nì´ ëª©í‘œ ìˆ˜ì§‘ëŸ‰: {sum(kw['posts_to_collect'] for kw in search_keywords)}ê°œ")
    print("="*70)
    
    all_posts_by_keyword = {}
    all_posts_combined = []
    seen_ids = set()
    
    for keyword_info in search_keywords:
        query = keyword_info['query']
        target_count = keyword_info['posts_to_collect']
        keyword_posts = []
        
        print(f"\n[{keyword_info['rank']}ìˆœìœ„] '{query}' ê²€ìƒ‰ ì¤‘... (ëª©í‘œ: {target_count}ê°œ)")
        
        # ë‹¤ì–‘í•œ ì •ë ¬ ë°©ì‹ìœ¼ë¡œ ê²€ìƒ‰í•˜ì—¬ ëª©í‘œ ê°œìˆ˜ ë‹¬ì„±
        for sort_method in ['hot', 'relevance', 'top']:
            if len(keyword_posts) >= target_count:
                break
                
            try:
                posts = reddit.search_posts(query, limit=20, sort=sort_method)
                
                for post in posts:
                    if len(keyword_posts) >= target_count:
                        break
                        
                    if post.post_id not in seen_ids:
                        seen_ids.add(post.post_id)
                        
                        try:
                            submission = reddit.reddit.submission(id=post.post_id)
                            
                            # ì ìˆ˜ 20 ì´ìƒì¸ ê²Œì‹œë¬¼ë§Œ
                            if submission.score >= 20:
                                # ëŒ“ê¸€ ìˆ˜ì§‘ (ìƒìœ„ 5ê°œ)
                                submission.comments.replace_more(limit=0)
                                top_comments = []
                                
                                comment_count = 0
                                for comment in sorted(submission.comments.list()[:20], 
                                                    key=lambda x: x.score if hasattr(x, 'score') else 0, 
                                                    reverse=True):
                                    if hasattr(comment, 'body') and comment_count < 5:
                                        top_comments.append({
                                            "author": str(comment.author) if comment.author else "[deleted]",
                                            "score": comment.score,
                                            "body": comment.body,
                                            "created_utc": comment.created_utc
                                        })
                                        comment_count += 1
                                
                                post_data = {
                                    "title": submission.title,
                                    "post_id": submission.id,
                                    "subreddit": submission.subreddit.display_name,
                                    "author": str(submission.author) if submission.author else "[deleted]",
                                    "score": submission.score,
                                    "upvote_ratio": submission.upvote_ratio,
                                    "num_comments": submission.num_comments,
                                    "created_utc": submission.created_utc,
                                    "url": f"https://reddit.com{submission.permalink}",
                                    "selftext": submission.selftext if submission.is_self else "",
                                    "link_url": submission.url if not submission.is_self else "",
                                    "top_comments": top_comments,
                                    "keyword_rank": keyword_info['rank'],
                                    "search_query": query,
                                    "sort_method": sort_method
                                }
                                
                                keyword_posts.append(post_data)
                                all_posts_combined.append(post_data)
                                print(f"  âœ“ [{submission.score}ì ] {submission.title[:50]}...")
                                
                        except Exception as e:
                            continue
                        
            except Exception as e:
                print(f"  âŒ ì˜¤ë¥˜: {e}")
            
            # API ì œí•œ ë°©ì§€
            time.sleep(0.5)
        
        # í‚¤ì›Œë“œë³„ ê²°ê³¼ ì €ì¥
        all_posts_by_keyword[query] = {
            "rank": keyword_info['rank'],
            "reason": keyword_info['reason'],
            "target_count": target_count,
            "actual_count": len(keyword_posts),
            "posts": sorted(keyword_posts, key=lambda x: x['score'], reverse=True)
        }
        
        print(f"  ìˆ˜ì§‘ ì™„ë£Œ: {len(keyword_posts)}ê°œ / ëª©í‘œ {target_count}ê°œ")
    
    # ì „ì²´ ê²Œì‹œë¬¼ ì ìˆ˜ìˆœ ì •ë ¬
    all_posts_combined.sort(key=lambda x: x['score'], reverse=True)
    
    # í†µê³„ ì •ë³´
    total_target = sum(kw['posts_to_collect'] for kw in search_keywords)
    print(f"\nğŸ“Š ì „ì²´ ìˆ˜ì§‘ í†µê³„:")
    print(f"  - ì´ ê²Œì‹œë¬¼: {len(all_posts_combined)}ê°œ")
    print(f"  - ëª©í‘œ ë‹¬ì„±ë¥ : {len(all_posts_combined)/total_target*100:.1f}%")
    
    print(f"\nğŸ“ í‚¤ì›Œë“œë³„ ìˆ˜ì§‘ ê²°ê³¼:")
    for query, data in all_posts_by_keyword.items():
        print(f"  [{data['rank']}ìˆœìœ„] {query}: {data['actual_count']}ê°œ/{data['target_count']}ê°œ")
    
    # ì €ì¥
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_data = {
        "topic": topic,
        "search_time": datetime.now().isoformat(),
        "total_posts": len(all_posts_combined),
        "collection_strategy": "í‚¤ì›Œë“œ ì¤‘ìš”ë„ë³„ ê°€ì¤‘ì¹˜ ì ìš©",
        "keywords_by_rank": search_keywords,
        "results_by_keyword": all_posts_by_keyword,
        "all_posts_combined": all_posts_combined
    }
    
    # ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ì—ì„œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì°¾ê¸°
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(os.path.dirname(script_dir))
    reports_dir = os.path.join(project_root, "reports", "raw_data")
    
    os.makedirs(reports_dir, exist_ok=True)
    # íŒŒì¼ëª…ì„ ì£¼ì œì— ë§ê²Œ ìƒì„± (ê³µë°±ì€ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€ê²½)
    safe_topic = topic.replace(' ', '_').replace(',', '_').lower()[:50]  # ìµœëŒ€ 50ì
    filename = os.path.join(reports_dir, f"weighted_search_{safe_topic}_{timestamp}.json")
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… ê°€ì¤‘ì¹˜ ê²€ìƒ‰ ì™„ë£Œ!")
    print(f"ğŸ’¾ ì €ì¥: {filename}")
    
    # TOP 5 ë¯¸ë¦¬ë³´ê¸°
    print("\nğŸ”¥ ì „ì²´ TOP 5 í•« ê²Œì‹œë¬¼:")
    for i, post in enumerate(all_posts_combined[:5], 1):
        print(f"\n{i}. [{post['score']}ì ] {post['title']}")
        print(f"   í‚¤ì›Œë“œ ìˆœìœ„: {post['keyword_rank']}ìˆœìœ„ | r/{post['subreddit']}")
    
    return filename

if __name__ == "__main__":
    # ëª…ë ¹ì¤„ ì¸ì í™•ì¸
    if len(sys.argv) > 1:
        user_input = sys.argv[1]
        weighted_search(user_input)
    else:
        weighted_search()