"""
Hacker News í¬ë¡¤ë§ ì„œë¹„ìŠ¤
ê°œë°œì ì»¤ë®¤ë‹ˆí‹°ì˜ ìµœì‹  íŠ¸ë Œë“œì™€ í† ë¡  ìˆ˜ì§‘
"""
from typing import List, Dict, Optional
from app.schemas.schemas import PostBase
import logging
import httpx
import asyncio
from datetime import datetime
from bs4 import BeautifulSoup
import json

logger = logging.getLogger(__name__)

class HackerNewsService:
    def __init__(self):
        self.base_url = "https://news.ycombinator.com"
        self.api_base = "https://hacker-news.firebaseio.com/v0"
        self.algolia_api = "https://hn.algolia.com/api/v1"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
    
    async def search_posts(self, query: str, limit: int = 25) -> List[PostBase]:
        """
        Hacker Newsì—ì„œ ê²Œì‹œë¬¼ ê²€ìƒ‰
        Algolia Search API ì‚¬ìš© (ê³µì‹ ê²€ìƒ‰ API)
        """
        posts = []
        
        try:
            # Algolia APIë¡œ ê²€ìƒ‰
            async with httpx.AsyncClient() as client:
                response = await client.get(
                    f"{self.algolia_api}/search",
                    params={
                        'query': query,
                        'tags': 'story',  # story, comment, poll ì¤‘ ì„ íƒ
                        'hitsPerPage': limit
                    }
                )
                
                if response.status_code == 200:
                    data = response.json()
                    hits = data.get('hits', [])
                    
                    for hit in hits:
                        post = self._parse_algolia_hit(hit)
                        if post:
                            posts.append(post)
            
            logger.info(f"Retrieved {len(posts)} posts from Hacker News for query: {query}")
            
        except Exception as e:
            logger.error(f"Error searching Hacker News: {e}")
        
        return posts
    
    def _parse_algolia_hit(self, hit: Dict) -> Optional[PostBase]:
        """Algolia ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹±"""
        try:
            # ì½˜í…ì¸  êµ¬ì„±
            content = ""
            
            # ë³¸ë¬¸ì´ ìˆëŠ” ê²½ìš° (Ask HN, Tell HN ë“±)
            if hit.get('story_text'):
                content = hit['story_text']
            elif hit.get('comment_text'):
                content = hit['comment_text']
            else:
                content = hit.get('title', '')
            
            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            metadata = f"\n\n---\n"
            metadata += f"ğŸ‘ Points: {hit.get('points', 0)} | "
            metadata += f"ğŸ’¬ Comments: {hit.get('num_comments', 0)} | "
            
            # ì‹œê°„ ì •ë³´
            created_at = hit.get('created_at')
            if created_at:
                metadata += f"ğŸ“… Posted: {created_at}"
            
            # URL ì²˜ë¦¬
            story_url = hit.get('url')  # ì™¸ë¶€ ë§í¬
            hn_url = f"{self.base_url}/item?id={hit.get('objectID', '')}"  # HN í† ë¡  ë§í¬
            
            content += metadata
            
            return PostBase(
                source="hackernews",
                post_id=hit.get('objectID'),
                author=hit.get('author', 'Unknown'),
                title=hit.get('title'),
                content=content,
                url=story_url or hn_url  # ì™¸ë¶€ ë§í¬ê°€ ìˆìœ¼ë©´ ìš°ì„ , ì—†ìœ¼ë©´ HN ë§í¬
            )
            
        except Exception as e:
            logger.error(f"Error parsing Algolia hit: {e}")
            return None
    
    async def get_top_stories(self, story_type: str = "top", limit: int = 30) -> List[PostBase]:
        """
        ì¸ê¸° ìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
        story_type: top, new, best, ask, show, job
        """
        posts = []
        
        try:
            async with httpx.AsyncClient() as client:
                # ìŠ¤í† ë¦¬ ID ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
                response = await client.get(f"{self.api_base}/{story_type}stories.json")
                
                if response.status_code == 200:
                    story_ids = response.json()[:limit]
                    
                    # ê° ìŠ¤í† ë¦¬ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ë³‘ë ¬ ì²˜ë¦¬)
                    tasks = []
                    for story_id in story_ids:
                        task = self._get_story_details(client, story_id)
                        tasks.append(task)
                    
                    stories = await asyncio.gather(*tasks, return_exceptions=True)
                    
                    for story in stories:
                        if isinstance(story, PostBase):
                            posts.append(story)
            
        except Exception as e:
            logger.error(f"Error getting top stories: {e}")
        
        return posts
    
    async def _get_story_details(self, client: httpx.AsyncClient, story_id: int) -> Optional[PostBase]:
        """ê°œë³„ ìŠ¤í† ë¦¬ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        try:
            response = await client.get(f"{self.api_base}/item/{story_id}.json")
            
            if response.status_code == 200:
                item = response.json()
                
                if not item or item.get('deleted') or item.get('dead'):
                    return None
                
                # ì½˜í…ì¸  êµ¬ì„±
                content = item.get('text', '') or item.get('title', '')
                
                # ë©”íƒ€ë°ì´í„°
                metadata = f"\n\n---\n"
                metadata += f"ğŸ‘ Score: {item.get('score', 0)} | "
                metadata += f"ğŸ’¬ Comments: {len(item.get('kids', []))} | "
                metadata += f"ğŸ“… Posted: {datetime.fromtimestamp(item.get('time', 0)).strftime('%Y-%m-%d %H:%M')}"
                
                content += metadata
                
                return PostBase(
                    source="hackernews",
                    post_id=str(story_id),
                    author=item.get('by', 'Unknown'),
                    title=item.get('title'),
                    content=content,
                    url=item.get('url') or f"{self.base_url}/item?id={story_id}"
                )
                
        except Exception as e:
            logger.error(f"Error getting story details: {e}")
            return None
    
    async def get_user_submissions(self, username: str, limit: int = 20) -> List[PostBase]:
        """íŠ¹ì • ì‚¬ìš©ìì˜ ì œì¶œë¬¼ ê°€ì ¸ì˜¤ê¸°"""
        posts = []
        
        try:
            async with httpx.AsyncClient() as client:
                # ì‚¬ìš©ì ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                response = await client.get(f"{self.api_base}/user/{username}.json")
                
                if response.status_code == 200:
                    user_data = response.json()
                    submitted_ids = user_data.get('submitted', [])[:limit]
                    
                    # ê° ì œì¶œë¬¼ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                    tasks = []
                    for item_id in submitted_ids:
                        task = self._get_story_details(client, item_id)
                        tasks.append(task)
                    
                    items = await asyncio.gather(*tasks, return_exceptions=True)
                    
                    for item in items:
                        if isinstance(item, PostBase):
                            posts.append(item)
                            
        except Exception as e:
            logger.error(f"Error getting user submissions: {e}")
        
        return posts
    
    async def get_comments_for_story(self, story_id: int, max_depth: int = 2) -> List[Dict]:
        """ìŠ¤í† ë¦¬ì˜ ëŒ“ê¸€ ê°€ì ¸ì˜¤ê¸°"""
        comments = []
        
        try:
            async with httpx.AsyncClient() as client:
                # ìŠ¤í† ë¦¬ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                response = await client.get(f"{self.api_base}/item/{story_id}.json")
                
                if response.status_code == 200:
                    story = response.json()
                    kid_ids = story.get('kids', [])
                    
                    # ëŒ“ê¸€ ê°€ì ¸ì˜¤ê¸° (ì¬ê·€ì ìœ¼ë¡œ)
                    for kid_id in kid_ids[:10]:  # ìƒìœ„ 10ê°œ ëŒ“ê¸€ë§Œ
                        comment = await self._get_comment_tree(client, kid_id, max_depth)
                        if comment:
                            comments.append(comment)
                            
        except Exception as e:
            logger.error(f"Error getting comments: {e}")
        
        return comments
    
    async def _get_comment_tree(self, client: httpx.AsyncClient, comment_id: int, 
                               max_depth: int, current_depth: int = 0) -> Optional[Dict]:
        """ëŒ“ê¸€ íŠ¸ë¦¬ êµ¬ì¡°ë¡œ ê°€ì ¸ì˜¤ê¸°"""
        if current_depth >= max_depth:
            return None
        
        try:
            response = await client.get(f"{self.api_base}/item/{comment_id}.json")
            
            if response.status_code == 200:
                comment = response.json()
                
                if not comment or comment.get('deleted') or comment.get('dead'):
                    return None
                
                comment_data = {
                    'id': comment_id,
                    'author': comment.get('by', 'Unknown'),
                    'text': comment.get('text', ''),
                    'time': datetime.fromtimestamp(comment.get('time', 0)).isoformat(),
                    'replies': []
                }
                
                # ëŒ€ëŒ“ê¸€ ê°€ì ¸ì˜¤ê¸°
                kid_ids = comment.get('kids', [])
                for kid_id in kid_ids[:3]:  # ëŒ€ëŒ“ê¸€ì€ 3ê°œê¹Œì§€ë§Œ
                    reply = await self._get_comment_tree(
                        client, kid_id, max_depth, current_depth + 1
                    )
                    if reply:
                        comment_data['replies'].append(reply)
                
                return comment_data
                
        except Exception as e:
            logger.error(f"Error getting comment tree: {e}")
            return None
    
    async def monitor_keywords(self, keywords: List[str], callback) -> None:
        """í‚¤ì›Œë“œ ëª¨ë‹ˆí„°ë§ (ì‹¤ì‹œê°„ ì•Œë¦¼)"""
        logger.info(f"Starting HN monitoring for keywords: {keywords}")
        
        last_seen_ids = set()
        
        while True:
            try:
                # ìƒˆ ìŠ¤í† ë¦¬ í™•ì¸
                new_stories = await self.get_top_stories("new", limit=30)
                
                for story in new_stories:
                    if story.post_id not in last_seen_ids:
                        last_seen_ids.add(story.post_id)
                        
                        # í‚¤ì›Œë“œ ë§¤ì¹­
                        content_lower = (story.title or '').lower() + ' ' + story.content.lower()
                        
                        for keyword in keywords:
                            if keyword.lower() in content_lower:
                                await callback(story, keyword)
                                break
                
                # ë©”ëª¨ë¦¬ ê´€ë¦¬ (ìµœê·¼ 1000ê°œë§Œ ìœ ì§€)
                if len(last_seen_ids) > 1000:
                    last_seen_ids = set(list(last_seen_ids)[-1000:])
                
                # 5ë¶„ë§ˆë‹¤ ì²´í¬
                await asyncio.sleep(300)
                
            except Exception as e:
                logger.error(f"Error in keyword monitoring: {e}")
                await asyncio.sleep(60)  # ì—ëŸ¬ ì‹œ 1ë¶„ ëŒ€ê¸°