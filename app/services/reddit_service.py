import praw
from typing import List, Dict, Optional
from app.core.config import settings
from app.schemas.schemas import PostBase
import logging
import asyncio
from datetime import datetime
import time
import ssl
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

logger = logging.getLogger(__name__)

class RedditService:
    def __init__(self):
        self.reddit = None
        if settings.REDDIT_CLIENT_ID and settings.REDDIT_CLIENT_SECRET:
            try:
                # SSL ê²€ì¦ì„ ë¹„í™œì„±í™”í•œ ì„¸ì…˜ ìƒì„±
                session = requests.Session()
                session.verify = False
                
                # SSL ê²½ê³  ë¹„í™œì„±í™”
                import urllib3
                urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
                
                self.reddit = praw.Reddit(
                    client_id=settings.REDDIT_CLIENT_ID,
                    client_secret=settings.REDDIT_CLIENT_SECRET,
                    user_agent=settings.REDDIT_USER_AGENT,
                    timeout=30,
                    requestor_kwargs={'session': session}
                )
                # ì—°ê²° í…ŒìŠ¤íŠ¸
                self.reddit.user.me()
                logger.info("âœ… Reddit í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                logger.error(f"âŒ Reddit í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def search_posts(self, query: str, limit: int = 25, sort: str = "relevance") -> List[PostBase]:
        """
        Redditì—ì„œ ê²Œì‹œë¬¼ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            limit: ê°€ì ¸ì˜¬ ê²Œì‹œë¬¼ ìˆ˜ (ìµœëŒ€ 100)
            sort: ì •ë ¬ ë°©ì‹ (relevance, hot, top, new)
        """
        if not self.reddit:
            logger.warning("Reddit client not initialized")
            return []
        
        posts = []
        try:
            # í•œê¸€ ê²€ìƒ‰ì–´ë¥¼ ìœ„í•œ í‚¤ì›Œë“œ ë§¤í•‘
            keyword_mapping = {
                "í…ŒìŠ¬ë¼": ["Tesla", "TSLA"],
                "ì• í”Œ": ["Apple", "AAPL"],
                "ì‚¼ì„±": ["Samsung"],
                "êµ¬ê¸€": ["Google", "GOOGL"],
                "ì•„ë§ˆì¡´": ["Amazon", "AMZN"],
                "ë©”íƒ€": ["Meta", "META", "Facebook"],
                "ì—”ë¹„ë””ì•„": ["NVIDIA", "NVDA"],
                "ë‰´ìŠ¤": ["news", "update", "announcement"],
                "ì£¼ì‹": ["stock", "shares", "investment"],
                "ì£¼ê°€": ["stock price", "share price"],
                "ì „ë§": ["forecast", "prediction", "outlook"],
                "ë¶„ì„": ["analysis", "review"],
            }
            
            # í•œê¸€ í‚¤ì›Œë“œë¥¼ ì˜ì–´ë¡œ ë³€í™˜
            search_queries = []
            original_query = query
            
            # í‚¤ì›Œë“œ ë§¤í•‘ ì ìš©
            for korean, english_list in keyword_mapping.items():
                if korean in query:
                    for eng in english_list:
                        modified_query = query.replace(korean, eng)
                        if modified_query not in search_queries:
                            search_queries.append(modified_query)
            
            # ì›ë³¸ ì¿¼ë¦¬ë„ ì¶”ê°€ (í˜¹ì‹œ ì˜ì–´ë¡œ ê²€ìƒ‰í•œ ê²½ìš°)
            if not search_queries or query == original_query:
                search_queries.append(query)
            
            logger.info(f"Original query: {original_query}")
            logger.info(f"Search queries: {search_queries}")
            
            # ê° ê²€ìƒ‰ì–´ë¡œ ê²€ìƒ‰ ì‹¤í–‰
            all_submissions = []
            seen_ids = set()
            
            for search_query in search_queries[:3]:  # ìµœëŒ€ 3ê°œ ì¿¼ë¦¬ë§Œ ì‹¤í–‰
                try:
                    logger.debug(f"Searching Reddit with query: {search_query}")
                    submissions = list(self.reddit.subreddit("all").search(
                        search_query, 
                        sort=sort, 
                        time_filter="week",  # ìµœê·¼ ì¼ì£¼ì¼
                        limit=min(limit, 100)
                    ))
                    
                    # ì¤‘ë³µ ì œê±°
                    for sub in submissions:
                        if sub.id not in seen_ids:
                            all_submissions.append(sub)
                            seen_ids.add(sub.id)
                    
                    logger.debug(f"Found {len(submissions)} posts for query: {search_query}")
                except Exception as e:
                    logger.error(f"Error searching with query '{search_query}': {e}")
            
            # ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ë” ë„“ì€ ë²”ìœ„ë¡œ ì¬ê²€ìƒ‰
            if not all_submissions and search_queries:
                logger.info("No results found, trying broader search...")
                # ì²« ë²ˆì§¸ ì˜ì–´ í‚¤ì›Œë“œë¡œë§Œ ì¬ê²€ìƒ‰
                first_english_word = None
                for word in search_queries[0].split():
                    if word.isascii() and len(word) > 2:
                        first_english_word = word
                        break
                
                if first_english_word:
                    logger.debug(f"Broader search with: {first_english_word}")
                    try:
                        submissions = list(self.reddit.subreddit("all").search(
                            first_english_word,
                            sort="hot",
                            time_filter="month",  # ë” ë„“ì€ ì‹œê°„ ë²”ìœ„
                            limit=min(limit, 50)
                        ))
                        all_submissions.extend(submissions)
                    except Exception as e:
                        logger.error(f"Error in broader search: {e}")
            
            # ê²€ìƒ‰ ê²°ê³¼ë¥¼ PostBase ê°ì²´ë¡œ ë³€í™˜
            for submission in all_submissions[:limit]:
                # ëŒ“ê¸€ ìˆ˜ì™€ ì ìˆ˜ ì •ë³´ ì¶”ê°€
                post = PostBase(
                    source="reddit",
                    post_id=submission.id,
                    author=str(submission.author) if submission.author else "[deleted]",
                    title=submission.title,
                    content=self._get_post_content(submission),
                    url=f"https://reddit.com{submission.permalink}",
                    # ë©”íƒ€ë°ì´í„° ì¶”ê°€
                    score=submission.score,
                    comments=submission.num_comments,
                    created_utc=submission.created_utc,
                    subreddit=submission.subreddit.display_name
                )
                posts.append(post)
                
            logger.info(f"ğŸ“‹ Reddit ê²€ìƒ‰ ì™„ë£Œ | í‚¤ì›Œë“œ: '{original_query}' | ê²°ê³¼: {len(posts)}ê°œ")
            
        except praw.exceptions.APIException as e:
            logger.error(f"âš ï¸ Reddit API ì˜¤ë¥˜: {e}")
        except Exception as e:
            logger.error(f"âŒ Reddit ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        
        return posts
    
    def search_subreddit(self, subreddit_name: str, query: str, limit: int = 25) -> List[PostBase]:
        """íŠ¹ì • ì„œë¸Œë ˆë”§ì—ì„œ ê²€ìƒ‰"""
        if not self.reddit:
            return []
        
        posts = []
        try:
            subreddit = self.reddit.subreddit(subreddit_name)
            submissions = list(subreddit.search(query, limit=limit))
            
            for submission in submissions:
                post = PostBase(
                    source=f"reddit/{subreddit_name}",
                    post_id=submission.id,
                    author=str(submission.author) if submission.author else "[deleted]",
                    title=submission.title,
                    content=self._get_post_content(submission),
                    url=f"https://reddit.com{submission.permalink}",
                    # ë©”íƒ€ë°ì´í„° ì¶”ê°€
                    score=submission.score,
                    comments=submission.num_comments,
                    created_utc=submission.created_utc,
                    subreddit=submission.subreddit.display_name
                )
                posts.append(post)
                
        except Exception as e:
            logger.error(f"Error searching subreddit {subreddit_name}: {e}")
        
        return posts
    
    def get_trending_topics(self, subreddits: List[str] = None) -> List[Dict]:
        """ì¸ê¸° í† í”½ ê°€ì ¸ì˜¤ê¸°"""
        if not self.reddit:
            return []
        
        if not subreddits:
            subreddits = ["technology", "programming", "machinelearning", "artificial"]
        
        trending = []
        try:
            for sub_name in subreddits:
                subreddit = self.reddit.subreddit(sub_name)
                hot_posts = list(subreddit.hot(limit=5))
                
                for post in hot_posts:
                    trending.append({
                        "subreddit": sub_name,
                        "title": post.title,
                        "score": post.score,
                        "comments": post.num_comments,
                        "url": f"https://reddit.com{post.permalink}"
                    })
                    
        except Exception as e:
            logger.error(f"Error getting trending topics: {e}")
        
        return trending
    
    def _get_post_content(self, submission) -> str:
        """ê²Œì‹œë¬¼ ë‚´ìš© ì¶”ì¶œ ë° í¬ë§·íŒ…"""
        content_parts = []
        
        # ë³¸ë¬¸
        if submission.selftext:
            content_parts.append(submission.selftext[:1000])
        
        # ë©”íƒ€ë°ì´í„°
        meta = f"\n\n---\n"
        meta += f"ğŸ‘ Score: {submission.score} | "
        meta += f"ğŸ’¬ Comments: {submission.num_comments} | "
        meta += f"ğŸ“… Posted: {datetime.fromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M')}"
        
        content_parts.append(meta)
        
        return "\n".join(content_parts)
    
    async def collect_reddit_posts(self, query: str, limit: int = 25) -> List[PostBase]:
        """ë¹„ë™ê¸° ë˜í¼ ë©”ì„œë“œ - ìŠ¤ì¼€ì¤„ëŸ¬ì—ì„œ ì‚¬ìš©"""
        # ë™ê¸° ë©”ì„œë“œë¥¼ ë¹„ë™ê¸°ë¡œ ë˜í•‘
        import asyncio
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self.search_posts, query, limit)