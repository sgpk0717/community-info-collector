import praw
from typing import List, Dict, Optional
from app.core.config import settings
from app.schemas.schemas import PostBase
import logging
import asyncio
from datetime import datetime
import time
import ssl
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import openai
import re

logger = logging.getLogger(__name__)

class RedditService:
    def __init__(self):
        self.reddit = None
        if settings.REDDIT_CLIENT_ID and settings.REDDIT_CLIENT_SECRET:
            try:
                # SSL ê²€ì¦ì„ ë¹„í™œì„±í™”í•œ ì„¸ì…˜ ìƒì„±
                session = requests.Session()
                session.verify = False
                
                # SSL ê²½ê³  ë¹„í™œì„±í™”
                import urllib3
                urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
                
                self.reddit = praw.Reddit(
                    client_id=settings.REDDIT_CLIENT_ID,
                    client_secret=settings.REDDIT_CLIENT_SECRET,
                    user_agent=settings.REDDIT_USER_AGENT,
                    timeout=30,
                    requestor_kwargs={'session': session}
                )
                # ì—°ê²° í…ŒìŠ¤íŠ¸
                self.reddit.user.me()
                logger.info("âœ… Reddit í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                logger.error(f"âŒ Reddit í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def search_posts(self, query: str, limit: int = 25, sort: str = "relevance") -> List[PostBase]:
        """
        Redditì—ì„œ ê²Œì‹œë¬¼ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            limit: ê°€ì ¸ì˜¬ ê²Œì‹œë¬¼ ìˆ˜ (ìµœëŒ€ 100)
            sort: ì •ë ¬ ë°©ì‹ (relevance, hot, top, new)
        """
        if not self.reddit:
            logger.warning("Reddit client not initialized")
            return []
        
        posts = []
        try:
            # í•œê¸€ ê²€ìƒ‰ì–´ë¥¼ ìœ„í•œ í‚¤ì›Œë“œ ë§¤í•‘ (í™•ì¥)
            keyword_mapping = {
                # ê¸°ì—…
                "í…ŒìŠ¬ë¼": ["Tesla", "TSLA"],
                "ì• í”Œ": ["Apple", "AAPL"],
                "ì‚¼ì„±": ["Samsung"],
                "êµ¬ê¸€": ["Google", "GOOGL"],
                "ì•„ë§ˆì¡´": ["Amazon", "AMZN"],
                "ë©”íƒ€": ["Meta", "META", "Facebook"],
                "ì—”ë¹„ë””ì•„": ["NVIDIA", "NVDA"],
                
                # ì¸ë¬¼
                "ì¼ëŸ°ë¨¸ìŠ¤í¬": ["Elon Musk", "Musk"],
                "íŠ¸ëŸ¼í”„": ["Trump", "Donald Trump"],
                "ë°”ì´ë“ ": ["Biden"],
                
                # ë‰´ìŠ¤/ì •ë³´
                "ë‰´ìŠ¤": ["news", "update", "announcement"],
                "ìµœì‹ ë‰´ìŠ¤": ["latest news", "breaking news", "recent news"],
                "ì†ë³´": ["breaking news", "breaking"],
                "ìµœì‹ ": ["latest", "recent", "new"],
                
                # ì£¼ì‹/ê¸ˆìœµ
                "ì£¼ì‹": ["stock", "shares", "investment"],
                "ì£¼ê°€": ["stock price", "share price"],
                "ì „ë§": ["forecast", "prediction", "outlook"],
                "ë¶„ì„": ["analysis", "review"],
                
                # ì—¬í–‰/ì¥ì†Œ
                "ë‹¤ë‚­": ["Da Nang", "Danang"],
                "í—¬ìŠ¤ì¥": ["gym", "fitness center"],
                "ìˆ™ì†Œ": ["hotel", "accommodation"],
                
                # ì‹œê°„
                "ì‹œê°„": ["hour", "hours"],
                "ì´ë‚´": ["within", "in"],
                "ìµœê·¼": ["recent", "latest"],
            }
            
            # í•œê¸€ í‚¤ì›Œë“œë¥¼ ì˜ì–´ë¡œ ë³€í™˜
            search_queries = []
            original_query = query
            
            # í‚¤ì›Œë“œ ë§¤í•‘ ì ìš©
            for korean, english_list in keyword_mapping.items():
                if korean in query:
                    for eng in english_list:
                        modified_query = query.replace(korean, eng)
                        if modified_query not in search_queries:
                            search_queries.append(modified_query)
            
            # ì›ë³¸ ì¿¼ë¦¬ë„ ì¶”ê°€ (í˜¹ì‹œ ì˜ì–´ë¡œ ê²€ìƒ‰í•œ ê²½ìš°)
            if not search_queries or query == original_query:
                search_queries.append(query)
            
            # í•œê¸€ì´ í¬í•¨ëœ ê²½ìš° OpenAIë¥¼ ì‚¬ìš©í•´ ë²ˆì—­
            if re.search('[ê°€-í£]', original_query) and len(search_queries) < 6:
                try:
                    openai.api_key = settings.OPENAI_API_KEY
                    response = openai.chat.completions.create(
                        model="gpt-4.1",
                        messages=[
                            {"role": "system", "content": "You are a translator. Translate the Korean search query to English. Provide 3 different translations that would work well for Reddit search. Return only the translations separated by '|' without any explanation."},
                            {"role": "user", "content": original_query}
                        ],
                        temperature=0.3,
                        max_tokens=100
                    )
                    
                    translations = response.choices[0].message.content.strip().split('|')
                    for trans in translations:
                        trans = trans.strip()
                        if trans and trans not in search_queries:
                            search_queries.append(trans)
                    
                    logger.info(f"OpenAI translations added: {translations}")
                except Exception as e:
                    logger.warning(f"OpenAI translation failed: {e}")
            
            # ìµœëŒ€ 6ê°œê¹Œì§€ë§Œ ì‚¬ìš©
            search_queries = search_queries[:6]
            
            logger.info(f"Original query: {original_query}")
            logger.info(f"Search queries: {search_queries}")
            
            # ê° ê²€ìƒ‰ì–´ë¡œ ê²€ìƒ‰ ì‹¤í–‰
            all_submissions = []
            seen_ids = set()
            
            for search_query in search_queries:  # ëª¨ë“  ì¿¼ë¦¬ ì‹¤í–‰ (ìµœëŒ€ 6ê°œ)
                try:
                    logger.debug(f"Searching Reddit with query: {search_query}")
                    submissions = list(self.reddit.subreddit("all").search(
                        search_query, 
                        sort=sort, 
                        time_filter="week",  # ìµœê·¼ ì¼ì£¼ì¼
                        limit=min(limit, 100)
                    ))
                    
                    # ì¤‘ë³µ ì œê±°
                    for sub in submissions:
                        if sub.id not in seen_ids:
                            all_submissions.append(sub)
                            seen_ids.add(sub.id)
                    
                    logger.debug(f"Found {len(submissions)} posts for query: {search_query}")
                except Exception as e:
                    logger.error(f"Error searching with query '{search_query}': {e}")
            
            # ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ë” ë„“ì€ ë²”ìœ„ë¡œ ì¬ê²€ìƒ‰
            if not all_submissions and search_queries:
                logger.info("No results found, trying broader search...")
                # ì²« ë²ˆì§¸ ì˜ì–´ í‚¤ì›Œë“œë¡œë§Œ ì¬ê²€ìƒ‰
                first_english_word = None
                for word in search_queries[0].split():
                    if word.isascii() and len(word) > 2:
                        first_english_word = word
                        break
                
                if first_english_word:
                    logger.debug(f"Broader search with: {first_english_word}")
                    try:
                        submissions = list(self.reddit.subreddit("all").search(
                            first_english_word,
                            sort="hot",
                            time_filter="month",  # ë” ë„“ì€ ì‹œê°„ ë²”ìœ„
                            limit=min(limit, 50)
                        ))
                        all_submissions.extend(submissions)
                    except Exception as e:
                        logger.error(f"Error in broader search: {e}")
            
            # ê²€ìƒ‰ ê²°ê³¼ë¥¼ PostBase ê°ì²´ë¡œ ë³€í™˜
            for submission in all_submissions[:limit]:
                # ëŒ“ê¸€ ìˆ˜ì™€ ì ìˆ˜ ì •ë³´ ì¶”ê°€
                post = PostBase(
                    source="reddit",
                    post_id=submission.id,
                    author=str(submission.author) if submission.author else "[deleted]",
                    title=submission.title,
                    content=self._get_post_content(submission),
                    url=f"https://reddit.com{submission.permalink}",
                    # ë©”íƒ€ë°ì´í„° ì¶”ê°€
                    score=submission.score,
                    comments=submission.num_comments,
                    created_utc=submission.created_utc,
                    subreddit=submission.subreddit.display_name
                )
                posts.append(post)
                
            logger.info(f"ğŸ“‹ Reddit ê²€ìƒ‰ ì™„ë£Œ | í‚¤ì›Œë“œ: '{original_query}' | ê²°ê³¼: {len(posts)}ê°œ")
            
        except praw.exceptions.APIException as e:
            logger.error(f"âš ï¸ Reddit API ì˜¤ë¥˜: {e}")
        except Exception as e:
            logger.error(f"âŒ Reddit ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        
        return posts
    
    def search_subreddit(self, subreddit_name: str, query: str, limit: int = 25) -> List[PostBase]:
        """íŠ¹ì • ì„œë¸Œë ˆë”§ì—ì„œ ê²€ìƒ‰"""
        if not self.reddit:
            return []
        
        posts = []
        try:
            subreddit = self.reddit.subreddit(subreddit_name)
            submissions = list(subreddit.search(query, limit=limit))
            
            for submission in submissions:
                post = PostBase(
                    source=f"reddit/{subreddit_name}",
                    post_id=submission.id,
                    author=str(submission.author) if submission.author else "[deleted]",
                    title=submission.title,
                    content=self._get_post_content(submission),
                    url=f"https://reddit.com{submission.permalink}",
                    # ë©”íƒ€ë°ì´í„° ì¶”ê°€
                    score=submission.score,
                    comments=submission.num_comments,
                    created_utc=submission.created_utc,
                    subreddit=submission.subreddit.display_name
                )
                posts.append(post)
                
        except Exception as e:
            logger.error(f"Error searching subreddit {subreddit_name}: {e}")
        
        return posts
    
    def get_trending_topics(self, subreddits: List[str] = None) -> List[Dict]:
        """ì¸ê¸° í† í”½ ê°€ì ¸ì˜¤ê¸°"""
        if not self.reddit:
            return []
        
        if not subreddits:
            subreddits = ["technology", "programming", "machinelearning", "artificial"]
        
        trending = []
        try:
            for sub_name in subreddits:
                subreddit = self.reddit.subreddit(sub_name)
                hot_posts = list(subreddit.hot(limit=5))
                
                for post in hot_posts:
                    trending.append({
                        "subreddit": sub_name,
                        "title": post.title,
                        "score": post.score,
                        "comments": post.num_comments,
                        "url": f"https://reddit.com{post.permalink}"
                    })
                    
        except Exception as e:
            logger.error(f"Error getting trending topics: {e}")
        
        return trending
    
    def _get_post_content(self, submission) -> str:
        """ê²Œì‹œë¬¼ ë‚´ìš© ì¶”ì¶œ ë° í¬ë§·íŒ…"""
        content_parts = []
        
        # ë³¸ë¬¸
        if submission.selftext:
            content_parts.append(submission.selftext[:1000])
        
        # ë©”íƒ€ë°ì´í„°
        meta = f"\n\n---\n"
        meta += f"ğŸ‘ Score: {submission.score} | "
        meta += f"ğŸ’¬ Comments: {submission.num_comments} | "
        meta += f"ğŸ“… Posted: {datetime.fromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M')}"
        
        content_parts.append(meta)
        
        return "\n".join(content_parts)
    
    async def collect_reddit_posts(self, query: str, limit: int = 25) -> List[PostBase]:
        """ë¹„ë™ê¸° ë˜í¼ ë©”ì„œë“œ - ìŠ¤ì¼€ì¤„ëŸ¬ì—ì„œ ì‚¬ìš©"""
        # ë™ê¸° ë©”ì„œë“œë¥¼ ë¹„ë™ê¸°ë¡œ ë˜í•‘
        import asyncio
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self.search_posts, query, limit)