#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ê³ ê¸‰ ê²€ìƒ‰ ì„œë¹„ìŠ¤ - weighted_search.py ê¸°ë°˜
"""
import json
import logging
import asyncio
from typing import List, Dict, Optional
from datetime import datetime
from openai import OpenAI
from app.core.config import settings
from app.services.reddit_service import RedditService
from app.schemas.schemas import PostBase
import time
import httpx

logger = logging.getLogger(__name__)

class AdvancedSearchService:
    def __init__(self):
        self.reddit_service = RedditService()
        self.openai_client = None
        if settings.OPENAI_API_KEY:
            # SSL ê²€ì¦ì„ ë¹„í™œì„±í™”í•œ HTTP í´ë¼ì´ì–¸íŠ¸ ìƒì„±
            http_client = httpx.Client(verify=False)
            self.openai_client = OpenAI(
                api_key=settings.OPENAI_API_KEY,
                http_client=http_client
            )
    
    async def expand_keywords_with_gpt4(self, user_input: str) -> List[Dict]:
        """GPT-4ë¥¼ ì‚¬ìš©í•˜ì—¬ í‚¤ì›Œë“œë¥¼ í™•ì¥í•˜ê³  ì˜ì–´ë¡œ ë³€í™˜"""
        if not self.openai_client:
            logger.warning("OpenAI client not initialized, using fallback")
            return [{"rank": 1, "query": self.translate_to_english_keywords(user_input), "posts_to_collect": 10, "reason": "ê¸°ë³¸ ë²ˆì—­"}]
        
        try:
            prompt = f"""
ì‚¬ìš©ìê°€ ì…ë ¥í•œ í‚¤ì›Œë“œ: "{user_input}"

ì´ í‚¤ì›Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ Redditì—ì„œ ìµœì‹  ì •ë³´ì™€ ì°Œë¼ì‹œ, ë£¨ë¨¸, ë‰´ìŠ¤ë¥¼ ì˜ ì°¾ì„ ìˆ˜ ìˆë„ë¡ 6ê°œì˜ ì˜ì–´ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”.

ìš”êµ¬ì‚¬í•­:
1. ì²« ë²ˆì§¸ í‚¤ì›Œë“œëŠ” ì›ë˜ í‚¤ì›Œë“œì˜ ì§ì ‘ ë²ˆì—­
2. ë‚˜ë¨¸ì§€ 5ê°œëŠ” ê´€ë ¨ëœ ìµœì‹  ì •ë³´, ë‰´ìŠ¤, ë£¨ë¨¸, ë¶„ì„ì„ ì˜ ì°¾ì„ ìˆ˜ ìˆëŠ” í‚¤ì›Œë“œ
3. ê° í‚¤ì›Œë“œëŠ” Reddit ê²€ìƒ‰ì— ì í•©í•˜ë„ë¡ 2-4ê°œ ë‹¨ì–´ë¡œ êµ¬ì„±
4. íˆ¬ì, ì£¼ì‹, ê¸°ì—… ê´€ë ¨ì´ë©´ earnings, forecast, analysis, news, rumor ë“± í¬í•¨
5. ì¸ë¬¼ ê´€ë ¨ì´ë©´ controversy, scandal, latest, update ë“± í¬í•¨

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{{
    "keywords": [
        {{"keyword": "í‚¤ì›Œë“œ1", "reason": "ì„ íƒ ì´ìœ "}},
        {{"keyword": "í‚¤ì›Œë“œ2", "reason": "ì„ íƒ ì´ìœ "}},
        {{"keyword": "í‚¤ì›Œë“œ3", "reason": "ì„ íƒ ì´ìœ "}},
        {{"keyword": "í‚¤ì›Œë“œ4", "reason": "ì„ íƒ ì´ìœ "}},
        {{"keyword": "í‚¤ì›Œë“œ5", "reason": "ì„ íƒ ì´ìœ "}},
        {{"keyword": "í‚¤ì›Œë“œ6", "reason": "ì„ íƒ ì´ìœ "}}
    ]
}}
"""
            
            response = self.openai_client.chat.completions.create(
                model="gpt-4.1",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7
            )
            
            # JSON ì‘ë‹µ íŒŒì‹±
            result = json.loads(response.choices[0].message.content)
            
            logger.info(f"GPT-4 generated expanded keywords for '{user_input}'")
            
            expanded_keywords = []
            for i, item in enumerate(result["keywords"], 1):
                expanded_keywords.append({
                    "rank": i,
                    "query": item["keyword"],
                    "posts_to_collect": 10,
                    "reason": item["reason"]
                })
            
            return expanded_keywords
            
        except Exception as e:
            logger.error(f"GPT-4 keyword expansion failed: {e}")
            # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë³€í™˜ ë¡œì§ ì‚¬ìš©
            return [{"rank": 1, "query": self.translate_to_english_keywords(user_input), "posts_to_collect": 10, "reason": "ê¸°ë³¸ ë²ˆì—­"}]
    
    def translate_to_english_keywords(self, korean_input: str) -> str:
        """í•œêµ­ì–´ í‚¤ì›Œë“œë¥¼ ì˜ì–´ë¡œ ë³€í™˜"""
        keyword_mapping = {
            "í…ŒìŠ¬ë¼": "Tesla",
            "ì• í”Œ": "Apple",
            "ì£¼ê°€": "stock price",
            "ì „ë§": "outlook forecast",
            "ì‚¼ì„±": "Samsung",
            "ë„¤ì´ë²„": "Naver",
            "ì¹´ì¹´ì˜¤": "Kakao",
            "LG": "LG",
            "í˜„ëŒ€": "Hyundai",
            "ê¸°ì•„": "Kia",
            "SK": "SK",
            "ë¶„ì„": "analysis",
            "íˆ¬ì": "investment",
            "ìˆ˜ìµ": "profit earnings",
            "ì‹¤ì ": "performance earnings",
            "ë§¤ì¶œ": "revenue sales",
            "ë°°ë‹¹": "dividend",
            "ìƒìŠ¹": "rise increase",
            "í•˜ë½": "fall decrease",
            "ì˜ˆì¸¡": "prediction forecast",
            "ë‰´ìŠ¤": "news",
            "ë°œí‘œ": "announcement",
            "ì‹¤ì ": "earnings results"
        }
        
        english_keywords = []
        for word in korean_input.split():
            if word in keyword_mapping:
                english_keywords.append(keyword_mapping[word])
            else:
                english_keywords.append(word)
        
        return " ".join(english_keywords)
    
    async def weighted_search(self, user_input: str, session_id: str = None) -> Dict:
        """ê°€ì¤‘ì¹˜ ê¸°ë°˜ ê²€ìƒ‰ - ì¤‘ìš”ë„ ìˆœìœ„ë³„ ê²Œì‹œë¬¼ ìˆ˜ì§‘"""
        if not self.reddit_service.reddit:
            logger.error("Reddit service not initialized")
            return {"posts": []}
        
        # GPT-4ë¥¼ ì‚¬ìš©í•˜ì—¬ í‚¤ì›Œë“œ í™•ì¥
        logger.info(f"Expanding keywords for: {user_input}")
        search_keywords = await self.expand_keywords_with_gpt4(user_input)
        
        logger.info(f"Generated {len(search_keywords)} search keywords")
        
        all_posts_by_keyword = {}
        all_posts_combined = []
        seen_ids = set()
        
        for keyword_info in search_keywords:
            query = keyword_info['query']
            target_count = keyword_info['posts_to_collect']
            keyword_posts = []
            
            logger.info(f"Searching for '{query}' (target: {target_count})")
            
            # ë‹¤ì–‘í•œ ì •ë ¬ ë°©ì‹ìœ¼ë¡œ ê²€ìƒ‰í•˜ì—¬ ëª©í‘œ ê°œìˆ˜ ë‹¬ì„±
            for sort_method in ['hot', 'relevance', 'top']:
                if len(keyword_posts) >= target_count:
                    break
                    
                try:
                    # ê¸°ì¡´ Reddit ì„œë¹„ìŠ¤ì˜ search_posts ì‚¬ìš©
                    posts = self.reddit_service.search_posts(query, limit=20, sort=sort_method)
                    
                    for post in posts:
                        if len(keyword_posts) >= target_count:
                            break
                            
                        if post.post_id not in seen_ids:
                            seen_ids.add(post.post_id)
                            
                            try:
                                submission = self.reddit_service.reddit.submission(id=post.post_id)
                                
                                # ì ìˆ˜ 20 ì´ìƒì¸ ê²Œì‹œë¬¼ë§Œ
                                if submission.score >= 20:
                                    # ëŒ“ê¸€ ìˆ˜ì§‘ (ìƒìœ„ 5ê°œ)
                                    submission.comments.replace_more(limit=0)
                                    top_comments = []
                                    
                                    comment_count = 0
                                    for comment in sorted(submission.comments.list()[:20], 
                                                        key=lambda x: x.score if hasattr(x, 'score') else 0, 
                                                        reverse=True):
                                        if hasattr(comment, 'body') and comment_count < 5:
                                            top_comments.append({
                                                "author": str(comment.author) if comment.author else "[deleted]",
                                                "score": comment.score,
                                                "body": comment.body,
                                                "created_utc": comment.created_utc
                                            })
                                            comment_count += 1
                                    
                                    # PostBase í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ë©”íƒ€ë°ì´í„° í¬í•¨)
                                    post_base = PostBase(
                                        source="reddit",
                                        post_id=submission.id,
                                        author=str(submission.author) if submission.author else "[deleted]",
                                        title=submission.title,
                                        content=self._format_post_content(submission, top_comments),
                                        url=f"https://reddit.com{submission.permalink}",
                                        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
                                        score=submission.score,
                                        comments=submission.num_comments,
                                        created_utc=submission.created_utc,
                                        subreddit=submission.subreddit.display_name
                                    )
                                    
                                    keyword_posts.append(post_base)
                                    all_posts_combined.append(post_base)
                                    
                                    logger.debug(f"Added post: [{submission.score}] {submission.title[:50]}...")
                                    
                            except Exception as e:
                                logger.error(f"Error processing post {post.post_id}: {e}")
                                continue
                            
                except Exception as e:
                    logger.error(f"Error searching with query '{query}': {e}")
                
                # API ì œí•œ ë°©ì§€
                await asyncio.sleep(0.5)
            
            # í‚¤ì›Œë“œë³„ ê²°ê³¼ ì €ì¥
            all_posts_by_keyword[query] = {
                "rank": keyword_info['rank'],
                "reason": keyword_info['reason'],
                "target_count": target_count,
                "actual_count": len(keyword_posts),
                "posts": keyword_posts
            }
            
            logger.info(f"Collected {len(keyword_posts)} posts for '{query}'")
        
        logger.info(f"Total posts collected: {len(all_posts_combined)}")
        
        return {
            "topic": user_input,
            "total_posts": len(all_posts_combined),
            "posts": all_posts_combined,
            "keywords_used": search_keywords,
            "results_by_keyword": all_posts_by_keyword
        }
    
    def _format_post_content(self, submission, top_comments: List[Dict]) -> str:
        """ê²Œì‹œë¬¼ ë‚´ìš© í¬ë§·íŒ…"""
        content_parts = []
        
        # ë³¸ë¬¸
        if submission.selftext:
            content_parts.append(submission.selftext[:1000])
        
        # ë©”íƒ€ë°ì´í„°
        meta = f"\n\n---\n"
        meta += f"ğŸ‘ Score: {submission.score} | "
        meta += f"ğŸ’¬ Comments: {submission.num_comments} | "
        meta += f"ğŸ“… Posted: {datetime.fromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M')}"
        
        content_parts.append(meta)
        
        # ìƒìœ„ ëŒ“ê¸€ ì¶”ê°€
        if top_comments:
            content_parts.append("\n\nğŸ”¥ Top Comments:")
            for i, comment in enumerate(top_comments[:3], 1):
                content_parts.append(f"{i}. [{comment['score']}] {comment['body'][:200]}...")
        
        return "\n".join(content_parts)