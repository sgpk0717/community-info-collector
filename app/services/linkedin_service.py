"""
LinkedIn ÌÅ¨Î°§ÎßÅ ÏÑúÎπÑÏä§
ÌîÑÎ°úÌïÑ, Ìè¨Ïä§Ìä∏, Ï±ÑÏö© Ï†ïÎ≥¥ ÏàòÏßë
"""
from typing import List, Dict, Optional
from app.schemas.schemas import PostBase
from app.services.browser_service import BrowserService
import logging
import asyncio
import re
from datetime import datetime
from bs4 import BeautifulSoup
import cloudscraper

logger = logging.getLogger(__name__)

class LinkedInService:
    def __init__(self):
        self.browser_service = BrowserService()
        self.base_url = "https://www.linkedin.com"
        self.scraper = cloudscraper.create_scraper()
        
    async def search_posts(self, query: str, limit: int = 25) -> List[PostBase]:
        """
        LinkedIn Í≤åÏãúÎ¨º Í≤ÄÏÉâ (Í≥µÍ∞ú ÏΩòÌÖêÏ∏†Îßå)
        Note: LinkedInÏùÄ Î°úÍ∑∏Ïù∏Ïù¥ ÌïÑÏöîÌïòÏßÄÎßå, ÏùºÎ∂Ä Í≥µÍ∞ú ÏΩòÌÖêÏ∏†Îäî Ï†ëÍ∑º Í∞ÄÎä•
        """
        posts = []
        
        try:
            # 1. Í≥µÍ∞ú Í≤ÄÏÉâ ÌéòÏù¥ÏßÄ ÏãúÎèÑ
            search_url = f"{self.base_url}/search/results/content/?keywords={query}"
            
            # Î∏åÎùºÏö∞Ï†Ä ÏÑúÎπÑÏä§ ÏÇ¨Ïö©
            await self.browser_service.start()
            
            # Í≥µÍ∞ú ÌîÑÎ°úÌïÑÍ≥º ÌöåÏÇ¨ ÌéòÏù¥ÏßÄÏóêÏÑú Ï†ïÎ≥¥ ÏàòÏßë
            company_posts = await self._search_company_posts(query)
            posts.extend(company_posts)
            
            # Ìï¥ÏãúÌÉúÍ∑∏ Í≤ÄÏÉâ
            hashtag_posts = await self._search_hashtag(query.replace(" ", ""))
            posts.extend(hashtag_posts)
            
            logger.info(f"Retrieved {len(posts)} posts from LinkedIn for query: {query}")
            
        except Exception as e:
            logger.error(f"Error searching LinkedIn: {e}")
        finally:
            await self.browser_service.stop()
        
        return posts[:limit]
    
    async def _search_company_posts(self, query: str) -> List[PostBase]:
        """ÌöåÏÇ¨ ÌéòÏù¥ÏßÄÏóêÏÑú Í≥µÍ∞ú Í≤åÏãúÎ¨º Í≤ÄÏÉâ"""
        posts = []
        
        try:
            # ÌöåÏÇ¨ Í≤ÄÏÉâ URL
            company_search_url = f"{self.base_url}/search/results/companies/?keywords={query}"
            
            page = await self.browser_service.create_stealth_page()
            await page.goto(company_search_url, wait_until='domcontentloaded')
            await asyncio.sleep(3)
            
            # ÌöåÏÇ¨ ÎßÅÌÅ¨ Ï∂îÏ∂ú
            company_links = await page.query_selector_all('a[href*="/company/"]')
            company_urls = []
            
            for link in company_links[:5]:  # ÏÉÅÏúÑ 5Í∞ú ÌöåÏÇ¨
                href = await link.get_attribute('href')
                if href and '/company/' in href:
                    company_urls.append(href)
            
            await page.close()
            
            # Í∞Å ÌöåÏÇ¨ ÌéòÏù¥ÏßÄÏóêÏÑú Í≤åÏãúÎ¨º ÏàòÏßë
            for company_url in company_urls:
                company_posts = await self._extract_company_posts(company_url)
                posts.extend(company_posts)
                
        except Exception as e:
            logger.error(f"Error searching company posts: {e}")
        
        return posts
    
    async def _extract_company_posts(self, company_url: str) -> List[PostBase]:
        """ÌäπÏ†ï ÌöåÏÇ¨ ÌéòÏù¥ÏßÄÏóêÏÑú Í≤åÏãúÎ¨º Ï∂îÏ∂ú"""
        posts = []
        
        try:
            page = await self.browser_service.create_stealth_page()
            await page.goto(company_url, wait_until='domcontentloaded')
            await asyncio.sleep(2)
            
            # ÌöåÏÇ¨Î™Ö Ï∂îÏ∂ú
            company_name = await page.text_content('h1')
            if not company_name:
                company_name = "Unknown Company"
            
            # Í≤åÏãúÎ¨º Ïª®ÌÖåÏù¥ÎÑà Ï∞æÍ∏∞
            post_containers = await page.query_selector_all('[data-urn*="activity"], .feed-shared-update-v2')
            
            for container in post_containers[:10]:
                try:
                    # ÌÖçÏä§Ìä∏ ÎÇ¥Ïö©
                    text_element = await container.query_selector('.feed-shared-text, .break-words')
                    content = ""
                    if text_element:
                        content = await text_element.text_content()
                    
                    if not content:
                        continue
                    
                    # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞
                    time_element = await container.query_selector('time')
                    timestamp = ""
                    if time_element:
                        timestamp = await time_element.get_attribute('datetime')
                    
                    # ÏÉÅÌò∏ÏûëÏö© Ïàò
                    reactions = await container.query_selector('[data-test-social-counts]')
                    engagement = ""
                    if reactions:
                        engagement = await reactions.text_content()
                    
                    post = PostBase(
                        source=f"linkedin/{company_name.lower().replace(' ', '_')}",
                        post_id=None,
                        author=company_name,
                        title=None,
                        content=f"{content.strip()}\n\n---\nüíº Company: {company_name}\n{engagement}",
                        url=company_url
                    )
                    posts.append(post)
                    
                except Exception as e:
                    logger.error(f"Error extracting post: {e}")
            
            await page.close()
            
        except Exception as e:
            logger.error(f"Error extracting company posts: {e}")
        
        return posts
    
    async def _search_hashtag(self, hashtag: str) -> List[PostBase]:
        """Ìï¥ÏãúÌÉúÍ∑∏Î°ú Í≤ÄÏÉâ"""
        posts = []
        
        try:
            # Ìï¥ÏãúÌÉúÍ∑∏ URL
            hashtag_url = f"{self.base_url}/feed/hashtag/{hashtag.lower()}/"
            
            page = await self.browser_service.create_stealth_page()
            await page.goto(hashtag_url, wait_until='domcontentloaded')
            await asyncio.sleep(2)
            
            # Í∞ÑÎã®Ìïú Ï†ïÎ≥¥ Ï∂îÏ∂ú
            content = await page.content()
            soup = BeautifulSoup(content, 'html.parser')
            
            # Î©îÌÉÄ ÌÉúÍ∑∏ÏóêÏÑú Ï†ïÎ≥¥ Ï∂îÏ∂ú
            description_meta = soup.find('meta', {'property': 'og:description'})
            if description_meta:
                post = PostBase(
                    source=f"linkedin/#{hashtag}",
                    post_id=None,
                    author="LinkedIn Hashtag",
                    title=f"#{hashtag}",
                    content=description_meta.get('content', ''),
                    url=hashtag_url
                )
                posts.append(post)
            
            await page.close()
            
        except Exception as e:
            logger.error(f"Error searching hashtag: {e}")
        
        return posts
    
    async def get_job_postings(self, query: str, location: str = "") -> List[Dict]:
        """Ï±ÑÏö© Í≥µÍ≥† Í≤ÄÏÉâ"""
        jobs = []
        
        try:
            # Ï±ÑÏö© Í≤ÄÏÉâ URL
            jobs_url = f"{self.base_url}/jobs/search/?keywords={query}"
            if location:
                jobs_url += f"&location={location}"
            
            page = await self.browser_service.create_stealth_page()
            await page.goto(jobs_url, wait_until='domcontentloaded')
            await asyncio.sleep(3)
            
            # Ï±ÑÏö© Í≥µÍ≥† Ïπ¥Îìú Ï∂îÏ∂ú
            job_cards = await page.query_selector_all('.job-card-container, [data-job-id]')
            
            for card in job_cards[:20]:
                try:
                    job = {}
                    
                    # ÏßÅÎ¨¥Î™Ö
                    title_elem = await card.query_selector('h3, .job-card-list__title')
                    if title_elem:
                        job['title'] = await title_elem.text_content()
                    
                    # ÌöåÏÇ¨Î™Ö
                    company_elem = await card.query_selector('h4, .job-card-container__company-name')
                    if company_elem:
                        job['company'] = await company_elem.text_content()
                    
                    # ÏúÑÏπò
                    location_elem = await card.query_selector('.job-card-container__metadata-item')
                    if location_elem:
                        job['location'] = await location_elem.text_content()
                    
                    # ÎßÅÌÅ¨
                    link_elem = await card.query_selector('a')
                    if link_elem:
                        href = await link_elem.get_attribute('href')
                        job['url'] = f"{self.base_url}{href}" if href.startswith('/') else href
                    
                    if job.get('title') and job.get('company'):
                        jobs.append(job)
                        
                except Exception as e:
                    logger.error(f"Error extracting job: {e}")
            
            await page.close()
            
        except Exception as e:
            logger.error(f"Error getting job postings: {e}")
        
        return jobs
    
    async def extract_profile_info(self, profile_url: str) -> Optional[Dict]:
        """Í≥µÍ∞ú ÌîÑÎ°úÌïÑ Ï†ïÎ≥¥ Ï∂îÏ∂ú"""
        try:
            page = await self.browser_service.create_stealth_page()
            await page.goto(profile_url, wait_until='domcontentloaded')
            await asyncio.sleep(2)
            
            profile = {}
            
            # Ïù¥Î¶Ñ
            name_elem = await page.query_selector('h1')
            if name_elem:
                profile['name'] = await name_elem.text_content()
            
            # Ìó§ÎìúÎùºÏù∏
            headline_elem = await page.query_selector('.text-body-medium')
            if headline_elem:
                profile['headline'] = await headline_elem.text_content()
            
            # ÏúÑÏπò
            location_elem = await page.query_selector('.text-body-small.inline')
            if location_elem:
                profile['location'] = await location_elem.text_content()
            
            # ÏÜåÍ∞ú
            about_elem = await page.query_selector('#about ~ div .inline-show-more-text')
            if about_elem:
                profile['about'] = await about_elem.text_content()
            
            await page.close()
            return profile
            
        except Exception as e:
            logger.error(f"Error extracting profile: {e}")
            return None