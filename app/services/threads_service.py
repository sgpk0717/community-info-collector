import httpx
from typing import List, Dict, Optional
from app.schemas.schemas import PostBase
import logging
from bs4 import BeautifulSoup
import asyncio
import json
import re
from datetime import datetime
import urllib.parse

logger = logging.getLogger(__name__)

class ThreadsService:
    def __init__(self):
        self.base_url = "https://www.threads.net"
        self.api_base = "https://www.threads.net/api/graphql"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        self.session_headers = {
            'X-IG-App-ID': '238260118697367',  # Threads app ID
            'Content-Type': 'application/x-www-form-urlencoded',
        }
    
    async def search_posts(self, query: str, limit: int = 25) -> List[PostBase]:
        """
        Threadsì—ì„œ ê²Œì‹œë¬¼ ê²€ìƒ‰ (ì›¹ ìŠ¤í¬ë˜í•‘)
        
        Note: ThreadsëŠ” ê³µì‹ APIê°€ ì—†ì–´ì„œ ì›¹ ìŠ¤í¬ë˜í•‘ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        Instagram Graph APIë¥¼ í†µí•œ ì ‘ê·¼ë„ ì œí•œì ì…ë‹ˆë‹¤.
        """
        posts = []
        
        try:
            # URL ì¸ì½”ë”©
            encoded_query = urllib.parse.quote(query)
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                # 1. ë¨¼ì € ì¼ë°˜ ì›¹ ê²€ìƒ‰ ì‹œë„
                posts_from_web = await self._search_web(client, query, limit)
                posts.extend(posts_from_web)
                
                # 2. ì‚¬ìš©ì ê²€ìƒ‰ë„ ì‹œë„ (ì‚¬ìš©ìëª…ì´ í¬í•¨ëœ ê²½ìš°)
                if "@" in query:
                    username = query.split("@")[1].split()[0]
                    user_posts = await self._get_user_posts(client, username, limit=10)
                    posts.extend(user_posts)
                
                logger.info(f"Retrieved {len(posts)} posts from Threads for query: {query}")
                
        except httpx.TimeoutException:
            logger.error("Threads request timed out")
        except Exception as e:
            logger.error(f"Error searching Threads: {e}")
        
        return posts[:limit]  # ì œí•œëœ ìˆ˜ë§Œ ë°˜í™˜
    
    async def _search_web(self, client: httpx.AsyncClient, query: str, limit: int) -> List[PostBase]:
        """ì›¹ ê²€ìƒ‰ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘"""
        posts = []
        
        try:
            # Threads ê²€ìƒ‰ URL (í•´ì‹œíƒœê·¸ ê²€ìƒ‰)
            if query.startswith("#"):
                tag = query[1:]
                url = f"{self.base_url}/t/{tag}"
            else:
                # ì¼ë°˜ ê²€ìƒ‰ì€ ì œí•œì ì´ë¯€ë¡œ ì¸ê¸° ê²Œì‹œë¬¼ì—ì„œ ê´€ë ¨ ë‚´ìš© ì°¾ê¸°
                url = self.base_url
            
            response = await client.get(url, headers=self.headers, follow_redirects=True)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # JSON-LD ë°ì´í„° ì¶”ì¶œ ì‹œë„
                scripts = soup.find_all('script', type='application/ld+json')
                for script in scripts:
                    try:
                        data = json.loads(script.string)
                        if isinstance(data, list):
                            for item in data:
                                if item.get('@type') == 'SocialMediaPosting':
                                    post = self._parse_json_ld_post(item)
                                    if post and query.lower() in post.content.lower():
                                        posts.append(post)
                    except:
                        continue
                
                # ë©”íƒ€ íƒœê·¸ì—ì„œ ì •ë³´ ì¶”ì¶œ
                meta_posts = self._extract_from_meta_tags(soup, query)
                posts.extend(meta_posts)
                
            else:
                logger.warning(f"Threads web search returned status: {response.status_code}")
                
        except Exception as e:
            logger.error(f"Error in Threads web search: {e}")
        
        return posts
    
    async def _get_user_posts(self, client: httpx.AsyncClient, username: str, limit: int = 10) -> List[PostBase]:
        """íŠ¹ì • ì‚¬ìš©ìì˜ ê²Œì‹œë¬¼ ê°€ì ¸ì˜¤ê¸°"""
        posts = []
        
        try:
            url = f"{self.base_url}/@{username}"
            response = await client.get(url, headers=self.headers, follow_redirects=True)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ì‚¬ìš©ì í”„ë¡œí•„ í˜ì´ì§€ì—ì„œ ê²Œì‹œë¬¼ ì¶”ì¶œ
                # ThreadsëŠ” ë™ì  ë¡œë”©ì„ ì‚¬ìš©í•˜ë¯€ë¡œ ì´ˆê¸° ë¡œë“œëœ ê²Œì‹œë¬¼ë§Œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŒ
                post_containers = soup.find_all('div', {'role': 'article'})
                
                for container in post_containers[:limit]:
                    post = self._parse_post_container(container, username)
                    if post:
                        posts.append(post)
                        
            else:
                logger.warning(f"Failed to get user posts for @{username}")
                
        except Exception as e:
            logger.error(f"Error getting user posts: {e}")
        
        return posts
    
    def _parse_json_ld_post(self, data: dict) -> Optional[PostBase]:
        """JSON-LD í˜•ì‹ì˜ ê²Œì‹œë¬¼ íŒŒì‹±"""
        try:
            author = data.get('author', {})
            author_name = author.get('name', 'Unknown')
            if isinstance(author, dict) and 'identifier' in author:
                author_name = f"@{author['identifier']['value']}"
            
            content = data.get('articleBody', '')
            url = data.get('url', '')
            
            if not content:
                return None
            
            # ë‚ ì§œ ì •ë³´ ì¶”ê°€
            date_published = data.get('datePublished', '')
            if date_published:
                content += f"\n\n---\nğŸ“… Posted: {date_published}"
            
            return PostBase(
                source="threads",
                post_id=url.split('/')[-1] if url else None,
                author=author_name,
                title=None,
                content=content[:1000],
                url=url
            )
        except Exception as e:
            logger.error(f"Error parsing JSON-LD post: {e}")
            return None
    
    def _parse_post_container(self, container, username: str) -> Optional[PostBase]:
        """HTML ì»¨í…Œì´ë„ˆì—ì„œ ê²Œì‹œë¬¼ íŒŒì‹±"""
        try:
            # í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ
            text_elements = container.find_all(['span', 'div'], string=True)
            content = ' '.join([elem.text.strip() for elem in text_elements if elem.text.strip()])
            
            if not content:
                return None
            
            # ë§í¬ ì¶”ì¶œ
            link = container.find('a', href=re.compile(r'/t/'))
            url = f"{self.base_url}{link['href']}" if link else None
            post_id = link['href'].split('/')[-1] if link else None
            
            return PostBase(
                source="threads",
                post_id=post_id,
                author=f"@{username}",
                title=None,
                content=content[:1000],
                url=url
            )
        except Exception as e:
            logger.error(f"Error parsing post container: {e}")
            return None
    
    def _extract_from_meta_tags(self, soup: BeautifulSoup, query: str) -> List[PostBase]:
        """ë©”íƒ€ íƒœê·¸ì—ì„œ ê´€ë ¨ ì •ë³´ ì¶”ì¶œ"""
        posts = []
        
        try:
            # Open Graph ë©”íƒ€ íƒœê·¸ í™•ì¸
            og_description = soup.find('meta', property='og:description')
            if og_description and og_description.get('content'):
                content = og_description['content']
                if query.lower() in content.lower():
                    # ê°„ë‹¨í•œ ë©”íƒ€ ì •ë³´ë¡œ ê²Œì‹œë¬¼ ìƒì„±
                    posts.append(PostBase(
                        source="threads",
                        post_id=None,
                        author="Threads",
                        title=None,
                        content=content,
                        url=self.base_url
                    ))
        except Exception as e:
            logger.error(f"Error extracting from meta tags: {e}")
        
        return posts
    
    async def get_trending_topics(self) -> List[Dict]:
        """Threads íŠ¸ë Œë”© í† í”½ (ì œí•œì )"""
        trending = []
        
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.get(self.base_url, headers=self.headers)
                
                if response.status_code == 200:
                    # í˜„ì¬ ThreadsëŠ” ê³µê°œì ì¸ íŠ¸ë Œë”© í˜ì´ì§€ê°€ ì—†ìŒ
                    # ëŒ€ì‹  ì¸ê¸° í•´ì‹œíƒœê·¸ë¥¼ ì°¾ì•„ë³¼ ìˆ˜ ìˆìŒ
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # í•´ì‹œíƒœê·¸ íŒ¨í„´ ì°¾ê¸°
                    hashtags = re.findall(r'#\w+', response.text)
                    hashtag_counts = {}
                    
                    for tag in hashtags:
                        hashtag_counts[tag] = hashtag_counts.get(tag, 0) + 1
                    
                    # ìƒìœ„ í•´ì‹œíƒœê·¸
                    for tag, count in sorted(hashtag_counts.items(), key=lambda x: x[1], reverse=True)[:10]:
                        trending.append({
                            'name': tag,
                            'url': f"{self.base_url}/t/{tag[1:]}",
                            'mentions': count
                        })
                        
        except Exception as e:
            logger.error(f"Error getting Threads trending topics: {e}")
        
        return trending