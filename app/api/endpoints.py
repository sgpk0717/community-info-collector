from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks, Query
from typing import List, Optional, Dict
from app.schemas.schemas import SearchRequest, SearchResponse, PostResponse, ReportResponse
from app.services.reddit_service import RedditService
from app.services.twitter_service import TwitterService
from app.services.threads_service import ThreadsService
from app.services.llm_service import LLMService
from app.services.progress_service import progress_service
from app.services.supabase_reports_service import supabase_reports_service
from app.services.push_notification_service import push_notification_service
import asyncio
import logging
from datetime import datetime
import uuid

logger = logging.getLogger(__name__)
router = APIRouter()

# ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
reddit_service = RedditService()
twitter_service = TwitterService()
threads_service = ThreadsService()
llm_service = LLMService()

@router.post("/search", response_model=SearchResponse)
async def search_and_analyze(
    request: SearchRequest, 
    background_tasks: BackgroundTasks
):
    """
    ì—¬ëŸ¬ ì»¤ë®¤ë‹ˆí‹°ì—ì„œ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³  ë¶„ì„í•©ë‹ˆë‹¤.
    
    - **query**: ê²€ìƒ‰í•  í‚¤ì›Œë“œ
    - **sources**: ê²€ìƒ‰í•  í”Œë«í¼ ëª©ë¡ (reddit, twitter, threads)
    - **user_nickname**: ì‚¬ìš©ì ë‹‰ë„¤ì„
    - **schedule_yn**: ìŠ¤ì¼€ì¤„ë§ ì—¬ë¶€ (Y/N)
    - **schedule_period**: ì£¼ê¸° (ë¶„ ë‹¨ìœ„) - schedule_ynì´ Yì¼ ë•Œ í•„ìˆ˜
    - **schedule_count**: ë°˜ë³µ íšŸìˆ˜ - schedule_ynì´ Yì¼ ë•Œ í•„ìˆ˜  
    - **schedule_start_time**: ì‹œì‘ ì‹œê°„ - schedule_ynì´ Yì¼ ë•Œ í•„ìˆ˜
    """
    # ì„¸ì…˜ ID ì‚¬ìš© (í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì „ë‹¬í•˜ê±°ë‚˜ ìƒˆë¡œ ìƒì„±)
    session_id = request.session_id or str(uuid.uuid4())
    
    logger.info(f"=== Search API Called ===")
    logger.info(f"Session ID: {session_id}")
    logger.info(f"Request data: {request.dict()}")
    logger.info(f"Query: {request.query}")
    logger.info(f"Sources: {request.sources}")
    logger.info(f"User nickname: {request.user_nickname}")
    logger.info(f"Schedule YN: {request.schedule_yn}")
    
    # ì§„í–‰ ìƒíƒœ ì´ˆê¸°í™”
    await progress_service.update_progress(
        session_id, 
        "initializing", 
        5, 
        "ë¶„ì„ ì¤€ë¹„ ì¤‘...",
        "ìš”ì²­ì„ ì²˜ë¦¬í•˜ê³  ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤"
    )
    # ìŠ¤ì¼€ì¤„ë§ íŒŒë¼ë¯¸í„° ê²€ì¦
    if request.schedule_yn == "Y":
        if not all([request.schedule_period, request.schedule_count, request.schedule_start_time]):
            raise HTTPException(
                status_code=400,
                detail="ìŠ¤ì¼€ì¤„ë§ì´ í™œì„±í™”ëœ ê²½ìš° schedule_period, schedule_count, schedule_start_timeì´ í•„ìˆ˜ì…ë‹ˆë‹¤."
            )
        
        # ìŠ¤ì¼€ì¤„ë§ ìƒì„± ë¡œì§
        from app.services.supabase_schedule_service import supabase_schedule_service
        
        # ì‚¬ìš©ì ë‹‰ë„¤ì„ìœ¼ë¡œ user_id ì¡°íšŒ
        if request.user_nickname:
            from app.services.supabase_service import supabase_service
            user_result = await supabase_service.get_user_by_nickname(request.user_nickname)
            if user_result and user_result["success"]:
                user_data = user_result["data"]
                # ìŠ¤ì¼€ì¤„ ìƒì„±
                schedule_data = {
                    "user_id": user_data.get("id"),
                    "keyword": request.query,
                    "interval_minutes": request.schedule_period,
                    "total_reports": request.schedule_count,
                    "report_length": request.length.value,
                    "next_run": request.schedule_start_time,
                    "sources": request.sources
                }
                schedule = await supabase_schedule_service.create_schedule(schedule_data)
                logger.info(f"ìŠ¤ì¼€ì¤„ ìƒì„±ë¨: {schedule.id}")
            else:
                raise HTTPException(
                    status_code=404,
                    detail="ì‚¬ìš©ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                )
    
    # ê¸°ì¡´ ê²€ìƒ‰ ë¡œì§ì€ ê·¸ëŒ€ë¡œ ìœ ì§€
    # ê²€ìƒ‰ ì¿¼ë¦¬ëŠ” Supabase reportsì— ì§ì ‘ ì €ì¥ë˜ë¯€ë¡œ ë³„ë„ ì €ì¥ ë¶ˆí•„ìš”
    logger.info(f"Processing search query: {request.query}")
    query_id = str(uuid.uuid4())  # ì„ì‹œ query ID ìƒì„±
    
    # ê³ ê¸‰ ê°€ì¤‘ì¹˜ ê²€ìƒ‰ ì‹œìŠ¤í…œ ì‚¬ìš©
    if "reddit" in request.sources:
        await progress_service.update_progress(
            session_id, 
            "keyword_expansion", 
            20, 
            "ğŸ¤– AIê°€ í‚¤ì›Œë“œë¥¼ í™•ì¥í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
            f"'{request.query}'ë¥¼ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ìƒì„± ì¤‘"
        )
        
        logger.info("Using advanced weighted search system for Reddit")
        try:
            # GPT-4ë¡œ í‚¤ì›Œë“œ í™•ì¥
            from app.services.advanced_search_service import AdvancedSearchService
            advanced_search = AdvancedSearchService()
            
            await progress_service.update_progress(
                session_id, 
                "reddit_search", 
                40, 
                "ğŸ” Redditì—ì„œ ê²Œì‹œë¬¼ì„ ìˆ˜ì§‘í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
                "ê°€ì¤‘ì¹˜ ê¸°ë°˜ ê²€ìƒ‰ìœ¼ë¡œ ê³ í’ˆì§ˆ ê²Œì‹œë¬¼ì„ ì„ ë³„ ì¤‘"
            )
            
            # ê°€ì¤‘ì¹˜ ê¸°ë°˜ ê²€ìƒ‰ ì‹¤í–‰
            search_result = await advanced_search.weighted_search(request.query, session_id)
            all_posts = search_result.get('posts', [])
            
            logger.info(f"Advanced search completed. Total posts: {len(all_posts)}")
            
            await progress_service.update_progress(
                session_id, 
                "data_collection", 
                60, 
                f"ğŸ“Š {len(all_posts)}ê°œì˜ ê²Œì‹œë¬¼ì„ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤",
                "ëŒ“ê¸€ê³¼ ë©”íƒ€ë°ì´í„°ë¥¼ í¬í•¨í•œ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ"
            )
            
        except Exception as e:
            logger.error(f"Advanced search failed, falling back to basic search: {e}")
            # ê¸°ë³¸ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±
            all_posts = reddit_service.search_posts(request.query)
    else:
        # ê¸°ì¡´ ë¡œì§ ìœ ì§€ (Twitter, Threads)
        tasks = []
        logger.info(f"Starting search on platforms: {request.sources}")
        
        if "twitter" in request.sources:
            logger.info("Adding Twitter search task")
            tasks.append(asyncio.to_thread(twitter_service.search_posts, request.query))
        if "threads" in request.sources:
            logger.info("Adding Threads search task")
            tasks.append(asyncio.to_thread(threads_service.search_posts, request.query))
        
        logger.info(f"Total tasks to execute: {len(tasks)}")
        
        # ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ì§‘
        all_results = await asyncio.gather(*tasks, return_exceptions=True)
        all_posts = []
        
        logger.info(f"Search results collected. Total results: {len(all_results)}")
        
        for i, result in enumerate(all_results):
            if isinstance(result, Exception):
                logger.error(f"Search error in task {i}: {result}")
                logger.error(f"Exception type: {type(result)}")
            else:
                logger.info(f"Task {i} returned {len(result)} posts")
                all_posts.extend(result)
    
    # ìˆ˜ì§‘ëœ ê²Œì‹œë¬¼ ì •ë¦¬ (ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì—†ì´ ë©”ëª¨ë¦¬ì—ë§Œ ìœ ì§€)
    saved_posts = all_posts  # ëª¨ë“  post_dataë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    logger.info(f"Collected {len(saved_posts)} posts")
    
    # ìˆ˜ì§‘ëœ ê²Œì‹œë¬¼ì´ ì—†ìœ¼ë©´ ì—ëŸ¬ ë°˜í™˜
    if not saved_posts:
        logger.warning(f"No posts collected for query: {request.query}")
        raise HTTPException(
            status_code=404,
            detail="ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ì‹œë„í•´ì£¼ì„¸ìš”."
        )
    
    # ê³ ê¸‰ ê²€ì¦ëœ ë¶„ì„ ì‹œìŠ¤í…œ ì‚¬ìš©
    await progress_service.update_progress(
        session_id, 
        "analysis", 
        80, 
        "ğŸ§  AIê°€ ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
        "GPT-4.1ì„ ì‚¬ìš©í•˜ì—¬ ê²€ì¦ëœ ë¶„ì„ ë³´ê³ ì„œ ìƒì„± ì¤‘"
    )
    
    try:
        from app.services.verified_analysis_service import VerifiedAnalysisService
        verified_analysis = VerifiedAnalysisService()
        
        # ê²€ì¦ëœ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±
        report_data = await verified_analysis.generate_verified_report(
            request.query,
            saved_posts,
            report_length=request.length.value if request.length else "moderate",
            session_id=session_id
        )
        
        logger.info("Used verified analysis system")
        
    except Exception as e:
        logger.error(f"Verified analysis failed, using basic LLM: {e}")
        # ê¸°ë³¸ LLM ì„œë¹„ìŠ¤ë¡œ í´ë°±
        report_data = await llm_service.generate_report(
            request.query, 
            saved_posts,
            report_length=request.length.value if request.length else "moderate"
        )
    
    # Report ê°ì²´ ìƒì„± (ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì—†ì´ ì„ì‹œ ìƒì„±)
    report_id = str(uuid.uuid4())
    
    # LLMì´ ìƒì„±í•œ post_mappings ì‚¬ìš© (ìˆìœ¼ë©´)
    posts_metadata = report_data.get("post_mappings", [])
    
    # post_mappingsê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
    if not posts_metadata:
        for post in saved_posts[:30]:  # ìµœëŒ€ 30ê°œ í¬ìŠ¤íŠ¸ì˜ ë©”íƒ€ë°ì´í„° ì €ì¥
            if post.url:  # URLì´ ìˆëŠ” í¬ìŠ¤íŠ¸ë§Œ
                metadata = {
                    "url": post.url,
                    "score": post.score,
                    "comments": post.comments,
                    "created_utc": post.created_utc,
                    "subreddit": post.subreddit,
                    "title": post.title[:100] if post.title else None
                }
                posts_metadata.append(metadata)
    
    # Supabaseì— ë³´ê³ ì„œ ì €ì¥
    try:
        supabase_report_data = {
            "user_nickname": request.user_nickname,
            "query_text": request.query,
            "full_report": report_data["full_report"],
            "summary": report_data["summary"],
            "posts_collected": len(saved_posts),
            "report_length": request.length.value if request.length else "moderate",
            "session_id": session_id,
            "posts_metadata": posts_metadata  # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        }
        
        save_result = await supabase_reports_service.save_report(supabase_report_data)
        if save_result["success"]:
            logger.info(f"Report saved to Supabase: {save_result['report_id']}")
            
            # í‘¸ì‹œ ì•Œë¦¼ ì „ì†¡ (ë¹„ë™ê¸°)
            if request.push_token:
                asyncio.create_task(
                    push_notification_service.send_analysis_complete_notification(
                        push_token=request.push_token,
                        user_nickname=request.user_nickname or "ì‚¬ìš©ì",
                        keyword=request.query,
                        report_id=save_result['report_id']
                    )
                )
        else:
            logger.error(f"Failed to save report to Supabase: {save_result['error']}")
    except Exception as e:
        logger.error(f"Error saving report to Supabase: {e}")
    
    # ì™„ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸
    await progress_service.update_progress(
        session_id, 
        "completed", 
        100, 
        "âœ… ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!",
        f"ì´ {len(saved_posts)}ê°œ ê²Œì‹œë¬¼ ë¶„ì„ ì™„ë£Œ"
    )
    
    response = SearchResponse(
        query_id=query_id,
        query_text=request.query,
        posts_collected=len(saved_posts),
        report=ReportResponse(
            id=report_id,
            search_query_id=query_id,
            summary=report_data["summary"],
            full_report=report_data["full_report"],
            created_at=datetime.utcnow()
        )
    )
    
    # ì‘ë‹µì— ì„¸ì…˜ ID ì¶”ê°€
    response.session_id = session_id
    
    return response

@router.get("/search/{query_id}", response_model=SearchResponse)
async def get_search_details(query_id: int):
    """íŠ¹ì • ê²€ìƒ‰ì˜ ìƒì„¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    # ì´ ì—”ë“œí¬ì¸íŠ¸ëŠ” Supabase ê¸°ë°˜ìœ¼ë¡œ ì¬êµ¬í˜„ í•„ìš”
    raise HTTPException(
        status_code=501,
        detail="ì´ ì—”ë“œí¬ì¸íŠ¸ëŠ” í˜„ì¬ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Supabase ê¸°ë°˜ APIë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."
    )

@router.get("/posts/{query_id}", response_model=List[PostResponse])
async def get_posts_by_query(
    query_id: int, 
    source: Optional[str] = Query(None, description="í•„í„°ë§í•  ì†ŒìŠ¤ (reddit, twitter, threads)"),
    limit: int = Query(50, description="ë°˜í™˜í•  ìµœëŒ€ ê²Œì‹œë¬¼ ìˆ˜")
):
    """ê²€ìƒ‰ ì¿¼ë¦¬ì˜ ìˆ˜ì§‘ëœ ê²Œì‹œë¬¼ë“¤ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    # ì´ ì—”ë“œí¬ì¸íŠ¸ëŠ” Supabase ê¸°ë°˜ìœ¼ë¡œ ì¬êµ¬í˜„ í•„ìš”
    raise HTTPException(
        status_code=501,
        detail="ì´ ì—”ë“œí¬ì¸íŠ¸ëŠ” í˜„ì¬ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Supabase ê¸°ë°˜ APIë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."
    )

@router.get("/report/{query_id}", response_model=ReportResponse)
async def get_report_by_query(query_id: int):
    """ê²€ìƒ‰ ì¿¼ë¦¬ì˜ ë¶„ì„ ë³´ê³ ì„œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    # ì´ ì—”ë“œí¬ì¸íŠ¸ëŠ” Supabase ê¸°ë°˜ìœ¼ë¡œ ì¬êµ¬í˜„ í•„ìš”
    raise HTTPException(
        status_code=501,
        detail="ì´ ì—”ë“œí¬ì¸íŠ¸ëŠ” í˜„ì¬ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Supabase ê¸°ë°˜ APIë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."
    )

@router.get("/trending")
async def get_trending_topics():
    """ê° í”Œë«í¼ì˜ íŠ¸ë Œë”© í† í”½ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    trending = {}
    
    # Reddit íŠ¸ë Œë”©
    try:
        trending["reddit"] = await reddit_service.get_trending_topics()
    except Exception as e:
        logger.error(f"Error getting Reddit trending: {e}")
        trending["reddit"] = []
    
    # Twitter íŠ¸ë Œë”©
    try:
        trending["twitter"] = await twitter_service.get_trending_topics()
    except Exception as e:
        logger.error(f"Error getting Twitter trending: {e}")
        trending["twitter"] = []
    
    # Threads íŠ¸ë Œë”©
    try:
        trending["threads"] = await threads_service.get_trending_topics()
    except Exception as e:
        logger.error(f"Error getting Threads trending: {e}")
        trending["threads"] = []
    
    return trending

@router.get("/search/history")
async def get_search_history(
    limit: int = Query(10, description="ë°˜í™˜í•  ìµœëŒ€ ê²€ìƒ‰ ìˆ˜")
):
    """ìµœê·¼ ê²€ìƒ‰ ì´ë ¥ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    # ì´ ì—”ë“œí¬ì¸íŠ¸ëŠ” Supabase ê¸°ë°˜ìœ¼ë¡œ ì¬êµ¬í˜„ í•„ìš”
    raise HTTPException(
        status_code=501,
        detail="ì´ ì—”ë“œí¬ì¸íŠ¸ëŠ” í˜„ì¬ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Supabase ê¸°ë°˜ APIë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."
    )

@router.delete("/search/{query_id}")
async def delete_search(query_id: int):
    """ê²€ìƒ‰ ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ëª¨ë“  ë°ì´í„°ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤."""
    # ì´ ì—”ë“œí¬ì¸íŠ¸ëŠ” Supabase ê¸°ë°˜ìœ¼ë¡œ ì¬êµ¬í˜„ í•„ìš”
    raise HTTPException(
        status_code=501,
        detail="ì´ ì—”ë“œí¬ì¸íŠ¸ëŠ” í˜„ì¬ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Supabase ê¸°ë°˜ APIë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."
    )
    db.query(CollectedPost).filter(CollectedPost.search_query_id == query_id).delete()
    db.query(Report).filter(Report.search_query_id == query_id).delete()

@router.get("/stats")
async def get_statistics():
    """ì‹œìŠ¤í…œ í†µê³„ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    # ì´ ì—”ë“œí¬ì¸íŠ¸ëŠ” Supabase ê¸°ë°˜ìœ¼ë¡œ ì¬êµ¬í˜„ í•„ìš”
    raise HTTPException(
        status_code=501,
        detail="ì´ ì—”ë“œí¬ì¸íŠ¸ëŠ” í˜„ì¬ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Supabase ê¸°ë°˜ APIë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."
    )

@router.get("/reports/{user_nickname}")
async def get_user_reports(
    user_nickname: str,
    limit: int = Query(20, description="ë°˜í™˜í•  ìµœëŒ€ ë³´ê³ ì„œ ìˆ˜")
):
    """ì‚¬ìš©ìì˜ ë³´ê³ ì„œ ëª©ë¡ì„ Supabaseì—ì„œ ì¡°íšŒí•©ë‹ˆë‹¤."""
    try:
        result = await supabase_reports_service.get_user_reports(user_nickname, limit)
        
        if result["success"]:
            return {
                "success": True,
                "reports": result["data"],
                "count": result["count"]
            }
        else:
            raise HTTPException(
                status_code=500,
                detail=f"ë³´ê³ ì„œ ì¡°íšŒ ì‹¤íŒ¨: {result['error']}"
            )
    except Exception as e:
        logger.error(f"Error retrieving user reports: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/reports/detail/{report_id}")
async def get_report_detail(report_id: str):
    """íŠ¹ì • ë³´ê³ ì„œì˜ ìƒì„¸ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
    try:
        result = await supabase_reports_service.get_report_by_id(report_id)
        
        if result["success"]:
            return {
                "success": True,
                "report": result["data"]
            }
        else:
            raise HTTPException(
                status_code=404,
                detail="ë³´ê³ ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
    except Exception as e:
        logger.error(f"Error retrieving report detail: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/report/{report_id}/links")
async def get_report_links(report_id: str):
    """íŠ¹ì • ë³´ê³ ì„œì˜ ë§í¬ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
    try:
        result = await supabase_reports_service.get_report_links(report_id)
        
        if result["success"]:
            return {
                "success": True,
                "links": result["data"]
            }
        else:
            raise HTTPException(
                status_code=500,
                detail=f"ë§í¬ ì¡°íšŒ ì‹¤íŒ¨: {result['error']}"
            )
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë³´ê³ ì„œ ë§í¬ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail="ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

@router.delete("/reports/{report_id}")
async def delete_user_report(
    report_id: str,
    user_nickname: str = Query(description="ì‚­ì œë¥¼ ìš”ì²­í•˜ëŠ” ì‚¬ìš©ì ë‹‰ë„¤ì„")
):
    """ì‚¬ìš©ìì˜ ë³´ê³ ì„œë¥¼ ì‚­ì œí•©ë‹ˆë‹¤."""
    try:
        result = await supabase_reports_service.delete_report(report_id, user_nickname)
        
        if result["success"]:
            return {
                "success": True,
                "message": "ë³´ê³ ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."
            }
        else:
            raise HTTPException(
                status_code=403,
                detail="ë³´ê³ ì„œ ì‚­ì œ ê¶Œí•œì´ ì—†ê±°ë‚˜ ë³´ê³ ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
    except Exception as e:
        logger.error(f"Error deleting report: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/reports/{user_nickname}/stats")
async def get_user_report_stats(user_nickname: str):
    """ì‚¬ìš©ìì˜ ë³´ê³ ì„œ í†µê³„ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
    try:
        result = await supabase_reports_service.get_report_stats(user_nickname)
        
        if result["success"]:
            return {
                "success": True,
                "stats": result["data"]
            }
        else:
            raise HTTPException(
                status_code=500,
                detail=f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {result['error']}"
            )
    except Exception as e:
        logger.error(f"Error retrieving report stats: {e}")
        raise HTTPException(status_code=500, detail=str(e))